import os
import shutil
import time
from datetime import datetime, timedelta
import pandas as pd
import qrcode
import re
from flask import redirect, url_for, current_app, flash, after_this_request
from flask_login import current_user
from markupsafe import Markup
from sqlalchemy import and_, func, or_, Integer, cast
from string import ascii_uppercase
from wtforms.validators import DataRequired
from werkzeug.datastructures import FileStorage

from lims.queue import submit
from docxtpl import DocxTemplate, RichText
from docx2pdf import convert

from lims.cases.forms import Add
from lims.models import *
from lims import db, app
from lims.containers.functions import get_form_choices as get_container_choices
from lims.containers.functions import process_form as containers_process
from lims.containers.forms import Add as ContainerAdd
from lims.labels import print_label, fields_dict

from pathlib import Path
import tempfile

from dateutil import relativedelta
import numpy as np
from geopy.geocoders import Nominatim  # pip install geopy

import difflib
from lims.view_templates.views import add_item, get_values

geolocator = Nominatim(user_agent='geomapping', timeout=10)


def get_form_choices(form, function=None, agency_id=None, item=None):
    # Get case type choices
    case_types = CaseTypes.query.order_by(CaseTypes.accession_level.asc())
    if function == 'Add':
        case_types = case_types.filter(CaseTypes.code != 'PM')

    case_types = [(item.id, f'{item.code} | {item.name}') for item in case_types]
    form.case_type.choices = case_types
    form.case_type.choices.insert(0, (0, 'Please select case type'))

    # Get genders choices
    form.gender_id.choices = [(item.id, item.name) for item in Genders.query.all()]
    form.gender_id.choices.insert(0, (0, 'Please select gender'))
    # Get races choices
    form.race_id.choices = [(item.id, item.name) for item in Races.query.all()]
    form.race_id.choices.insert(0, (0, '---'))

    # Get submitting agency choices
    submitting_agencies = [(item.id, item.name) for item in
                           Agencies.query.join(Divisions).filter(Divisions.client == 'Yes').filter_by(
                               db_status='Active').order_by(Agencies.name.asc())]
    submitting_agencies.insert(0, (0, 'Please select an agency'))
    form.submitting_agency.choices = submitting_agencies

    # If agency_id, get divisions which are clients
    if not agency_id:
        agency_id = form.submitting_agency.data

    print(agency_id)
    if agency_id:
        divisions = [(item.id, item.name) for item in Divisions.query.filter_by(agency_id=agency_id, client='Yes')]
        divisions.insert(0, (0, 'Please select a submitting division'))
    else:
        divisions = [(0, 'No submitting agency selected')]

    form.submitting_division.choices = divisions

    # Disable the necessary fields
    if item:
        # Disable the age field if the item has both date_of_birth and date_of_incident
        if item.date_of_birth and item.date_of_incident:
            form.age.render_kw = {'disabled': True}

        # Disable the item.testing_requested field if there is no testing
        if not item.testing_requested:
            form.testing_requested.render_kw = {'disabled': True}
            form.no_testing_requested.render_kw = {'checked': True}
        else:
            form.no_testing_requested.render_kw = {'disabled': True}

    return form


def process_form(form, event=None, item=None):
    kwargs = {}
    # Get case type base on selection in form
    case_type_id = form.case_type.data
    case_type = CaseTypes.query.get(case_type_id)
    # If case number type is automatic (i.e., generated by increment)
    print(event)
    if event == 'Add':
        print(event)
        if case_type.case_number_type == 'Automatic':
            current_case_number = case_type.current_case_number  # Get current counter for case type
            kwargs[
                'case_number'] = f"{case_type.code}-{str(current_case_number).rjust(5, '0')}"  # Generate case number
            case_type.current_case_number += 1  # Increment
            db.session.commit()  # Commit changes
        else:
            kwargs['case_counter'] = ""
            kwargs['case_number'] = form.case_number.data
            # kwargs['ignore_fields'] += ['first_name', 'middle_name', 'last_name',
            #                             'date_of_birth', 'gender_id', 'birth_sex', 'race_id',
            #                             'date_of_incident', 'time_of_incident', 'submitting_agency']

        # Set number of containers in case to 0
        kwargs['n_containers'] = 0
        kwargs['retention_policy'] = case_type.retention_policy
        kwargs['discard_eligible'] = 'No'

    # Calculate age based on date_of_birth and date_of_incident
    dob = form.date_of_birth.data
    doi = form.date_of_incident.data
    if (dob and doi):
        age_dict = calculate_age(dob, doi)
        kwargs.update(age_dict)

    if form.age.data:
        kwargs['age_status'] = 'Known'

    ###

    kwargs['in_use'] = False

    ###

    testing_requested = []
    discipline_statuses = []

    kwargs['case_status'] = None

    for discipline in disciplines:

        discipline_requested = None
        discipline_performed = None
        discipline_status = None

        # If item has been passed to the process_form function (i.e., the case is being edited, approved or updated,
        # get the discipline status for the case. If it is already has a discipline, preserve that discipline.
        # This also prevents the removal of a discipline if it is already undergoing testing.
        if item:
            if getattr(item, f'{discipline.lower()}_status') not in [None, 'No Tests Added']:
                discipline_status = getattr(item, f'{discipline.lower()}_status')
                discipline_requested = getattr(item, f'{discipline.lower()}_requested')
                discipline_performed = getattr(item, f'{discipline.lower()}_performed')

        # If the discipline was selected in the form, add it the to the testing_requested list. This will populate
        # Cases.testing_requested.
        if discipline in form.testing_requested.data:
            # Set the default values for the discipline requested, performed and status. All disciplines that
            # are requested will be performed.
            discipline_requested = 'Yes'
            discipline_performed = 'Yes'
            # If there is no discipline
            if not discipline_status:
                discipline_status = 'No Tests Added'

        if discipline_performed:
            testing_requested.append(discipline)

            # Set the values
        kwargs[f'{discipline.lower()}_requested'] = discipline_requested
        kwargs[f'{discipline.lower()}_performed'] = discipline_performed
        kwargs[f'{discipline.lower()}_status'] = discipline_status

        # Get all the discipline statuses in order to determine the case status
        if discipline_status:
            discipline_statuses.append(discipline_status)

        # Join the selected disciplines
    kwargs['testing_requested'] = ", ".join(testing_requested)

    # if the case is being edited/approved/updated
    if item:
        specimens_approved = Specimens.query.filter_by(case_id=item.id, discipline=discipline, db_status='Active').count()
        all_specimens =  Specimens.query.filter(and_(Specimens.case_id == item.id, Specimens.discipline == discipline, 
                                                     Specimens.db_status != 'Removed')).count()
        # if there are no discipline statuses set the case status to None.
        if not discipline_statuses:
            kwargs['case_status'] = 'No Testing Requested'
        # if all discipline statuses which are being performed are 'Initiated' set the
        # case status to 'Queued'
        elif all(x == 'Initiated' for x in discipline_statuses):
            kwargs['case_status'] = 'Queued'
        # If any discipline status is 'No Tests Added', set the case status to 'Need Test Addition'.
        elif any(x == 'No Tests Added' for x in discipline_statuses) and \
            not Modifications.query.filter_by(table_name='Cases', record_id=str(item.id), status='Pending').count() and \
                specimens_approved == all_specimens:
            kwargs['case_status'] = 'Need Test Addition'
        # if any discipline status is in 'Testing', 'Drafting', 'CR', 'DR' set the case status to 'In Progress'
        elif any(x in ['Testing', 'Drafting', 'CR', 'DR'] for x in discipline_statuses):
            kwargs['case_status'] = 'In Progress'

    return kwargs


def calculate_age(dob=None, doi=None):
    """

    Parameters
    ----------
    dob : date of birth as a datetime object, default = None
    doi

    Returns
    -------
    Age of a person

    """
    age_years = None
    age_months = None
    age_days = None
    age_status = "Unknown"
    age = "Unknown"

    if doi and dob:
        age = ""
        try:
            delta = relativedelta.relativedelta(doi, dob)

            # if age_status in ['Approximated', 'Unknown']:
            #     age_str = '~'

            if delta.years >= 5:
                age += f'{delta.years}y'

            elif (delta.years < 5) and (delta.years >= 1):
                age += f'{delta.years}y, {delta.months}m'

            elif (delta.years < 1) and (delta.months > 0):
                age += f'{delta.months}m, {delta.days}d'

            elif (delta.years == 0) and (delta.months == 0):
                age += f'{delta.days}d'

            age_years = delta.years
            age_months = delta.months
            age_days = delta.days
            age_status = "Known"
        except:
            pass

    return {'age': age, 'age_years': age_years, 'age_months': age_months, 'age_days': age_days,
            'age_status': age_status}


def get_coordinates(address, zipcode):
    lat = np.nan
    long = np.nan

    # Remove this from the address
    remove_txt = ['ifo', 'in front of', 'across from', 'behind']

    # Remove the text after any of these, including the term
    stop_text = [', ', 'apartment', 'apt', '#', 'unit']

    if not pd.isnull(address):
        address = address.lower()
        # Remove the items in the "remove" list
        for x in remove_txt:
            address = address.replace(x, "")

        # If the address has a comma, "Apartment", "Apt", or "#" remove everything
        # from there on.
        for x in stop_text:
            if x in address:
                address = address[:address.find(x)]
            # Remove any white spaces on either side of the address
            address = address.strip()

        try:
            # Geolocate the center of the country
            city = geolocator.geocode(f"{zipcode}, US")[0]
            geocode = lambda address, city: geolocator.geocode(f"{address}, {city}")
            loc = geocode(address, city)
            # And return latitude and longitude
            lat = loc.latitude
            long = loc.longitude
        except:
            pass

    return lat, long


def get_personnel_id(personnel):
    last_name_split = personnel.split(", ")
    last_name = last_name_split[0]

    first_name_split = last_name_split[1].split(" ")
    first_name = first_name_split[0]

    personnel_id = Personnel.query.filter_by(last_name=last_name, first_name=first_name).first().id

    return personnel_id


# def get_distinct_values(dicts_list, key):  # for pt_evals
#     distinct_dict_list = []
#
#     seen_values = set()
#
#     for d in dicts_list:
#         value = d.get(key)
#         if value not in seen_values:
#             distinct_dict_list.append(d)
#             seen_values.add(value)
#
#     return distinct_dict_list
#
#
def get_to_dict(obj):  # for pt_evals
    return {c.key: getattr(obj, c.key) for c in obj.__table__.columns}


def get_autopsy_choices(form):
    form.cases_selected.choices = [(item.id, item.case_number) for item in Cases.query.filter_by(case_type=7)]
    form.cases_selected.choices.insert(0, (0, '---'))

    form.specimen_discipline.choices = [(x, y) for x, y in discipline_choices]

    return form


def add_new_container(container_type, form, submission_route_type, location_type, submission_route, discipline):
    """
    Used to automatically add a new container. This function fills out 'container_form' and 'submits' form to trigger
    add_item.
    Args:
        container_type (int): ID of container type
        form (form object): The form used to trigger this function
        submission_route_type (str): By Hand or By Location
        location_type (str): Name of location type table (e.g., Cooled Storage)
        submission_route (str): equipment_id of the actual item in the location type table
        discipline (str): Discipline associated with this container

    Returns:
        Adds new container and redirects to autopsy view and prints to autopsy printer
    """

    # Initialize alphabet array and accession_number
    alphabet = list(ascii_uppercase)
    accession_number = None
    kwargs = {}

    # Check if histology container being added
    if discipline == 'Histology':
        # Get case and check if containers already exist
        case = Cases.query.get(form.cases_selected.data)
        if not Containers.query.filter(and_(Containers.case_id == form.cases_selected.data,
                                            Containers.discipline == discipline)).count():
            # Create A0 Container
            accession_number = f'{case.case_number}_A0'

        else:
            # Get most recent container
            current_accession = Containers.query.filter(and_(Containers.case_id == form.cases_selected.data,
                                                             Containers.discipline == discipline)).order_by(
                Containers.accession_number.desc())[0]

            # Get current letter of container accession number
            current_letter = current_accession.accession_number[-2]

            # Find next letter in alphabet and set container accession_number
            counter = 0
            next_letter = ''
            for letter in alphabet:
                counter += 1
                if letter == current_letter:
                    next_letter = alphabet[counter]
                    break

            accession_number = f'{case.case_number}_{next_letter}0'

    # Check if container exists that user can add to and make sure discipline not Histology
    if not Containers.query.filter(and_(Containers.case_id == form.cases_selected.data,
                                        Containers.container_type_id == container_type,
                                        Containers.discipline == discipline,
                                        Containers.submission_time == None)).count() or discipline == 'Histology':

        # Create container
        container_form = get_container_choices(ContainerAdd(), location_type=location_type)

        # Assign case id based on choice
        container_form.case_id.data = form.cases_selected.data

        # Assign container discipline
        container_form.discipline.data = discipline

        # No container default
        container_form.container_type_id.data = container_type

        # The current users division assigned in personnel
        container_form.division_id.data = current_user.personnel.division_id

        # Current user as submitter
        container_form.submitted_by.data = current_user.personnel_id

        # Set to 0 and auto-increment with specimens submitted
        container_form.n_specimens_submitted.data = 0

        # Default to by location
        container_form.submission_route_type.data = submission_route_type

        # Default to Cooled Storage
        container_form.location_type.data = location_type

        # Default to 08R
        container_form.submission_route.data = submission_route

        # Set submission date to datetime now
        container_form.submission_date.data = datetime.now()

        # Remove validators for specific fields to force validation
        container_form.submission_time.validators = [v for v in container_form.submission_time.validators if
                                                     not isinstance(v, DataRequired)]
        container_form.n_specimens_submitted.validators = [v for v in
                                                           container_form.n_specimens_submitted.validators if
                                                           not isinstance(v, DataRequired)]
        # Set submit to True
        container_form.submit.data = True

        if container_form.is_submitted():
            # if a new container is added to the case, set the case to pending.
            case = Cases.query.get(container_form.case_id.data)
            case.pending_submitter = current_user.initials
            case.db_status = 'Pending'

            attributes_list = []

            # Process container form
            kwargs.update(containers_process(container_form, 'Add', accession_number))
            label_accession_number = kwargs['accession_number']
            add_item(container_form, Containers, 'Container', 'Containers', 'containers', True, 'accession_number',
                     **kwargs)
            print(f'LABEL ACC NUM: {label_accession_number}')
            container = Containers.query.filter_by(accession_number=label_accession_number).first()
            kwargs['container_id'] = container.id

            # Set container label attributes
            qr_path = os.path.join(current_app.root_path, 'static', 'label_qrs',
                                   f'container{container.id}.png')
            qrcode.make(f'containers: {container.id}').save(qr_path)
            label_attributes = fields_dict['container']
            label_attributes['CASE_NUM'] = case.case_number
            label_attributes['ACC_NUM'] = container.accession_number
            label_attributes['CODE'] = f'[{container.type.code}]'
            label_attributes['TYPE'] = container.type.name
            label_attributes['QR'] = qr_path
            label_attributes['DISCIPLINE'] = discipline

            # Do not print if no container
            if container_form.container_type_id.data == ContainerTypes.query.filter_by(name='No Container').first().id:
                print('DO NOT PRINT LABEL')
            else:
                # Print labels
                print('DONE PRINT LABEL HERE')
                if discipline == 'Histology':
                    for x in range(0, 2):
                        attributes_list.append(label_attributes.copy())

                else:
                    attributes_list.append(label_attributes.copy())

                print_label(r'\\OCMEG9M056.medex.sfgov.org\DYMO LabelWriter 450 Twin Turbo',
                            attributes_list, True, 1)
                print('FINISHED PRINTING')
                # print_label(r'DYMO LabelWriter 450 Twin Turbo (Copy 1)', attributes_list, True, 1)

    else:
        kwargs['container_id'] = Containers.query.filter(and_(Containers.case_id == form.cases_selected.data,
                                                              Containers.container_type_id == container_type,
                                                              Containers.discipline == discipline,
                                                              Containers.submission_time == None)).first().id

    print(f'KWARGS: {kwargs}')

    return kwargs


def chunks(lst, n):
    """Yield successive n-sized chunks from lst"""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


def get_ids(personnel):
    """

    Extracts the first_name and last_name from the Primary Pathologist
    field which is in the format <last_name>, <first_name> <title> (INV\ME).
    E.g., Taylor, Jordan MD (INV\ME)
    Parameters
    ----------
    personnel

    Returns
    -------
    personnel_id
    """

    personnel_id = None
    division_id = None
    id_num = None
    last_name = None
    first_name = None

    # remove comma and split the string on spaces
    if not pd.isnull(personnel):
        lst = personnel.replace(",", "").split(" ")

        # Some MEs have the middle initial listed in FA e.g., Guan, J. Jun MD (INV\ME)
        # If any of the parts of the name contain a '.' (essentially a middle initial) remove it.
        # This will also remove titles that contain '.' e.g., Ph.D. but the title is not needed to match
        # the ME to the personnel table.
        lst = [x for x in lst if '.' not in x]

        # The last_name is the first element in the list and the first_name is the second element.
        if len(lst) >= 3:
            last_name = lst[0]
            first_name = lst[1]
            raw_id = lst[2].lstrip("#")
            if raw_id.isdigit():
                id_num = int(raw_id)

        # Filter personnel table using last_name in addition to the
        # agency (SFOCME agency_id = 1)
        # if more than one share a last_name, include first name
        # if unsuccessful, filter by id_number
        personnel_matches = Personnel.query. \
            filter_by(last_name=last_name, agency_id=1).all()
        if len(personnel_matches) == 1:
            personnel = Personnel.query. \
                filter_by(last_name=last_name, agency_id=1).first()
        elif len(personnel_matches) > 1:
            personnel = Personnel.query. \
                filter_by(last_name=last_name, first_name=first_name, agency_id=1).first()
            if not personnel and id_num:
                personnel = Personnel.query. \
                    filter_by(id_number=id_num, agency_id=1).first()

        else:
            personnel = None

        if personnel:
            personnel_id = personnel.id
            division_id = personnel.division_id

    return personnel_id, division_id


import re
import html

def sanitize_text(text):
    
    text = html.unescape(text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    # text = text.lower()
    # text = re.sub(r'[^\w\s]', '', text)

    return text

# already has an uploaded Autopsy report (record_type = 1 corresponds to '_A' and has an attachment),
# and we haven't produced an O# record yet. This uses your existing queue.
def _has_autopsy_upload(case_id: int) -> bool:
    autopsy_reports = Records.query.filter(
        Records.case_id == case_id,
        Records.db_status != 'Removed',
        Records.record_type == 1            #(record_type = 1 corresponds to '_A')
    ).all()
    if not autopsy_reports:
        return False
    else:
        return True

def _has_decedent_record(case: Cases) -> bool:
    return bool(
        Records.query.filter(
            Records.case_id == case.id,
            Records.record_type == 14       #(record_type = 14 corresponds to '_O')
        ).first()
    )

# Helper: finalized COD_A = not empty and not 'PENDING' (any case/whitespace)
def _cod_a_finalized(val) -> bool:
    if val is None:
        return False
    s = str(val).strip()
    return bool(s) and s.upper() != 'PENDING'


def process_import_file(file, filename, savename, fa_col_map=None, impl_date=None, add_form=None):
    print("[FA IMPORT]>>>>>> ENTERED process_import_file")
    item_name = 'Cases'
    case_form_fields = [x.name for x in add_form]

    gender_dict = {item.name.lower(): item.id for item in Genders.query}
    race_dict = {item.name.lower(): item.id for item in Races.query}
    
    inv_dict = {"KRIS BARBRICH #114 (SF OCME)": r"Barbrich, Kris #114 (INV\ME)",
                "ADAM HELLMAN #109 (SF OCME)": r"Hellman, Adam #109 (INV\ME)",
                "JASAMYN WIMMER (SF OCME)": r"Wimmer, Jasamyn #112 (INV\ME)",
                "MARK POWNING #119 (SF OCME)": r"Powning, Mark #119 (INV\ME)",
                "WIL CHUAKAY #120 (SF OCME)": r"Chuakay, J. Wil #120 (INV\ME)",
                "NORMA CONTRERAS #115 (SF OCME)": r"Contreras, Norma #115 (INV\ME)",
                "JOE BEGOVICH #125 (SF OCME)": r"Begovich, Joseph #125 (INV\ME)",
                "RICKHI SEHMAR #127 (SF OCME)": r"Sehmar, Rickhi #127 (INV\ME)",
                "VERONICA VARGO (SF OCME)": r"Vargo, Veronica  (INV\ME)",
                "TRISH BUBNIS #116 (SF OCME)": r"Bubnis, Trish L #116 (INV\ME)",
                "KRIS MACFERREN #123 (SF OCME)": r"MacFerren, Kris W #123 (INV\ME)",
                r"Vargo, Veronica M (INV\ME)": r"Vargo, Veronica  (INV\ME)",
                r"Begovich, Joe #125 (INV\ME)": r"Begovich, Joseph #125 (INV\ME)"}
    
    ###### For FA 2020-2023 Data ######
    race_dict.update({
        'asian-chinese': 1,
        'asian-japanese': 1,
        'asian or pacific islander': 1,
        'asian-japanese': 1,
        'asian-filipino': 1,
        'other': 9
        })
    inv_dict.update({
        r"Kris Barbrich #114 (Other)": r"Barbrich, Kris #114 (INV\ME)",
        r"MARK POWNING #119 (SF OCME)": r"Powning, Mark #119 (INV\ME)",
        r"MICHAEL HOOGASIAN #121 (SF OCME)":r"Hoogasian, Michael #121 (INV\ME)",
        r"KRIS BARBRICH #114 (SF OCME)": r"Barbrich, Kris #114 (INV\ME)",
        r"ADAM HELLMAN #109 (SF OCME)": r"Hellman, Adam #109 (INV\ME)",
        r"TRISH BUBNIS #116 (SF OCME)": r"Bubnis, Trish L #116 (INV\ME)",
        r"RICKHI SEHMAR #127 (SF OCME)": r"Sehmar, Rickhi #127 (INV\ME)",
        r"KRIS MACFERREN #123 (SF OCME)": r"MacFerren, Kris W #123 (INV\ME)",
        r"ANTHONY MARCHINI #131 (SF OCME)":r"Marchini, Anthony #131 (INV\ME)",
        r"DEVIN ETHEREDGE #125 (SF OCME)":r"Etheredge, Devin  #125 (INV\ME)",
        r"MARK NAGAYO #112 (SF OCME)":r"Nagayo, Mark #112 (INV\ME)",
        r"ZACK SMITH #115 (SF OCME)":r"Smith, Zack #115 (INV\ME)",
        r"WIL CHUAKAY #120 (SF OCME)": r"Chuakay, J. Wil #120 (INV\ME)",
        r"JOE BEGOVICH #125 (SF OCME)": r"Begovich, Joseph #125 (INV\ME)",
        r"NORMA CONTRERAS #115 (SF OCME)": r"Contreras, Norma #115 (INV\ME)",
        r"MICHAEL SUCHOVICKI #117 (SF OCME)":r"Suchovicki, Ph.D., Michael #117 (INV\ME)",
        r"JASAMYN WIMMER (SF OCME)": r"Wimmer, Jasamyn #112 (INV\ME)",
        r"THOMAS MCDONALD #110 (SF OCME)":r"McDonald, Thomas #110 (INV\ME)"
    })
    ####################################
    
    case_type = CaseTypes.query.filter_by(name='Postmortem').first()
    hom_retention_policy_id = RetentionPolicies.query.filter_by(name='RERDP Sec. 8a').first().id
    non_retention_policy_id = case_type.retention.id

    column_mapping = pd.read_csv(fa_col_map)
    column_dict = dict(zip(column_mapping['FA Column'], column_mapping['LIMS Column']))
    column_dict['Latitude'] = 'latitude'
    column_dict['Longitude'] = 'longitude'
    narrative_cols = ['narrative_type', 'narrative']

    start_import = datetime.now()

    # This handles whether files have the export header from FA (currently the 4th row but can change!)
    # or the first row contains the columns of interest. This opens the file multiple times, increasing the number
    # of rows skipped until 'Case Number' in found in the column headers.
    for x in range(100):
        # Read csv and convert numerical columns to string
        cases = pd.read_csv(file, encoding_errors='ignore', skiprows=x,
                            dtype={'Home Zip': str,
                                   'Death Zip': str,
                                   'SSN': str,
                                   'Home Address1': str,
                                   'BodyRelTime': str,
                                   'NOKNotifTime': str
                                   })

        if 'CaseNumber' in cases.columns:
            break
    
    # Convert date columns to datetime (case sensitive to col names in FA Export)
    for col in cases.columns:
        if 'Date' in col:
            cases[col] = pd.to_datetime(cases[col], errors='ignore')

    # Remove No Case (NC) - leave in for manual FA import (if FA management report is used)
    cases = cases[cases['CaseType'] == 'Case']
    # Rename columns using column_dict
    cases = cases.rename(columns=column_dict)

    # Force HHMM strings for body_released_time / nok_notify_time (if present)
    for tcol in ("body_released_time", "nok_notify_time"):
        if tcol in cases.columns:
            cases[tcol] = (
                pd.to_datetime(cases[tcol], errors="coerce").dt.strftime("%H%M")
                .fillna(
                    cases[tcol].astype("string").str.strip()
                        .replace({"": None, "nan": None, "NaN": None, "None": None})
                        .str.replace(":", "", regex=False)
                )
            )


    # Get any columns not in the column_dict keys to remove
    drop_cols = [col for col in cases.columns if col not in column_dict.values()]

   # --- PERSONAL EFFECTS (prefixed-only) ---
    # Only accept columns that start with 'pe_' to avoid future collisions.
    PE_FIELDS = {
        'pe_type': 'type',
        'pe_quantity': 'quantity',
        'pe_description': 'description',
        'pe_disposition': 'disposition',
        'pe_received_by': 'received_by',
        'pe_received_date': 'received_date',
        'pe_released_to': 'released_to',
        'pe_released_date': 'released_date',
    }

    # Columns that actually exist in the import (prefixed-only)
    pe_present_cols = [c for c in PE_FIELDS.keys() if c in cases.columns]

    nartype_col = (
        'InvestigationNarrativeType'
        if 'InvestigationNarrativeType' in cases.columns
        else ('narrative_type' if 'narrative_type' in cases.columns else None)
    )

    # (Optional) warn if any unprefixed/bare names slipped in; they will be ignored.
    bare_hits = [b for b in ('type','quantity','description','disposition',
                            'received_by','received_date','released_to','released_date')
                if b in cases.columns]
    # if bare_hits:
    #     print(f"[FA IMPORT][WARN] Ignoring unexpected bare PE columns: {bare_hits}")

    if pe_present_cols:
        # make sure case_number exists OR alias a candidate before this block (shown below)
        # nt == Narrative Type
        nt_col = nartype_col  # 'InvestigationNarrativeType' or 'narrative_type' or None
        if not nt_col:
            # If we truly have no narrative type column, behave like before (no filtering).
            # Build from any row that has PE data.
            pe_src = cases[['case_number'] + pe_present_cols].copy()
            pe_src = pe_src.dropna(how='all', subset=pe_present_cols)
        else:
            # Normalize narrative type and assign priority (lower number = better)
            nt_norm = (
                cases[nt_col]
                .astype('string')
                .str.strip()
                .str.lower()
                .fillna('')
            )

            PRIORITY_ORDER = {
                'initial': 0,
                'summary': 1,
                'summary (ai)': 2,
                # everything else defaults to low priority (10)
            }
            pri = nt_norm.map(PRIORITY_ORDER).fillna(10)

            # Row-level: does this row actually contain any PE data?
            has_pe = cases[pe_present_cols].notna().any(axis=1)

            # For each case, find the best (lowest) priority that *has PE*
            pri_with_pe = pri.where(has_pe, 999)  # rows without PE become "very bad"
            min_pri_per_case = pri_with_pe.groupby(cases['case_number']).transform('min')

            # Keep only rows from the chosen narrative priority for that case, and only rows with PE
            preferred_rows = has_pe & (pri_with_pe == min_pri_per_case)

            # Build the PE source from those rows
            pe_src = cases.loc[preferred_rows, ['case_number'] + pe_present_cols].copy()

        # Light normalization so "  glasses  " and "glasses" don’t create false differences,
        # but DO NOT drop_duplicates here — we want legitimate duplicates preserved.
        for c in pe_present_cols:
            if pe_src[c].dtype == 'object':
                pe_src[c] = pe_src[c].astype('string').str.strip()

        # Normalize dates to datetime; the payload coercion later converts to Python datetime
        for c in ('pe_received_date', 'pe_released_date'):
            if c in pe_src.columns:
                pe_src[c] = pd.to_datetime(pe_src[c], errors='ignore')

        # Remove rows where *all* PE fields are blank/null (paranoia guard)
        pe_src = pe_src.dropna(how='all', subset=pe_present_cols)

        personal_effects_df = pe_src

        # We’ll later drop PE columns from the main 'cases' table; add to drop list now
        drop_cols += pe_present_cols
    else:
        # Safe empty frame; no dependency on cases' columns
        personal_effects_df = pd.DataFrame(columns=['case_number'] + pe_present_cols)

    # Extract the narratives
    narrative_df = cases[['case_number'] + narrative_cols].dropna(subset=['narrative'])
    # Add the narrative columns to the columns to be removed
    drop_cols += narrative_cols
    # Remove unused columns and duplicate cases (due to the inclusion of narratives
    # there is a row per narrative for each case
    cases = cases.drop(drop_cols, axis=1).drop_duplicates('case_number')
    # Replace True/False in Restrict Public Access
    cases['fa_restrict_public_access'].replace({True: 'Yes', False: np.nan}, inplace=True)

    

    dfs = {
        'cases': cases,
        'narratives': narrative_df,
        'personal_effects': personal_effects_df
    }

    # used to track imported cases for decedent report gen
    cases_touched = set()

    # For PE import idempotency (only clear once per case; only insert once)
    pe_cleared_cases = set()

    if current_user:
        current_initials = current_user.initials
        current_id = current_user.id
    else:
        current_initials = "ZZZ"
        current_id = 45


    # cases.to_excel('C:\\Users\\spearring\\Downloads\\import-milestone\\cases.xlsx', index=False)
    # narrative_df.to_excel('C:\\Users\\spearring\\Downloads\\import-milestone\\narratives.xlsx', index=False)
    # personal_effects_df.to_excel('C:\\Users\\spearring\\Downloads\\import-milestone\\personal_effects.xlsx', index=False)

    for df_name, df in dfs.items():
        for _, row in df.iterrows():
            is_case_newnarrative = False
            case_close_date = None
            item = False
            # Set default field values
            field_data = {
                'case_type': case_type.id,
                'create_date': datetime.now(),
                'created_by': current_initials,
                'db_status': 'Active',
            }

            # Check if the case already exists in LIMS
            case = Cases.query.filter_by(case_number=row['case_number']).first()
            has_A_imported = _has_autopsy_upload(case.id) if case else None
            imported_A_date = Records.query.filter(
                                Records.case_id==case.id,
                                Records.db_status!='Removed',
                                Records.record_type==1
                                ).first().fa_OR_importDate if has_A_imported else None

            if df_name == 'cases':
                is_case_newnarrative = True
                  # We touched this case during import (for potential triggers later)
                if 'case_number' in row and row['case_number']:
                    cases_touched.add(str(row['case_number']))

                table = Cases
                item_name = 'Cases'
                
                row['case_type'] = case_type.id

                # Set last name to upper case
                if not pd.isnull(row['last_name']):
                    row['last_name'] = row['last_name'].upper()
                # Remove dashes and white space in middle name column
                if not pd.isnull(row['middle_name']):
                    row['middle_name'] = row['middle_name'].replace("-", "").strip()
                # Set time_of_incident HHMM
                row['time_of_incident'] = np.nan
                if not pd.isnull(row['date_of_incident']):
                    row['time_of_incident'] = row['date_of_incident'].strftime("%H%M")
                # Set the submitting agency as SFOCME (agency_id = 1)
                row['submitting_agency'] = 1
                # Get the ids for the primary_pathologist and submitting_division
                # the submitting divisions is the primary pathologist's division.
                # i.e., if the primary pathologist is Christopher Liverman, then
                # the division is Medical Division (CL)
                row['primary_pathologist'], row['submitting_division'] = get_ids(row['primary_pathologist'])
                # Get the id for the person who certified the death certificate. Only need
                # personnel_id hence why getting the 0-index
                row['certified_by'] = get_ids(row['certified_by'])[0]
                # Get the id of the primary_investigator. Uses the same function as above
                # but only returns personnel_id
                row['primary_investigator'] = get_ids(row['primary_investigator'])[0]

                sec_inv = row.get('secondary_investigator')
                if sec_inv in inv_dict:
                    inv_sec = inv_dict[sec_inv]
                    row['secondary_investigator'] = get_ids(inv_sec)[0]
                elif sec_inv:  # if it's not None, empty, or Falsey
                    try:
                        row['secondary_investigator'] = get_ids(sec_inv)[0]
                    except:
                        row['secondary_investigator'] = None
                else:
                    row['secondary_investigator'] = None

                # Replace genders, races, zips
                if not pd.isnull(row['gender_id']):
                    row['gender_id'] = gender_dict[row['gender_id'].lower()]
                if not pd.isnull(row['race_id']):
                    row['race_id'] = race_dict[row['race_id'].lower()]

                if not case:
                    row['priority'] = 'Normal'
                    row['sensitivity'] = 'Normal'
                    row['discard_eligible'] = 'No'
                    if row['fa_case_entry_date'].date() >= impl_date:
                        row['testing_requested'] = 'Toxicology'
                        row['toxicology_requested'] = 'Yes'
                        row['toxicology_performed'] = 'Yes'
                        row['toxicology_status'] = 'No Tests Added'

                    # To speed up the import process, only calculate coordinates
                    # if the case does not exist or the case does not already have coordinates or
                    # the death address is different to an already existing case.

                    # if 'latitude' exists in the import file it implies the cases have already gone
                    # through the process of calculating coordinates.
                    if 'latitude' not in row:
                        row['latitude'], row['longitude'] = get_coordinates(row['death_address'],
                                                                            row['death_zip'])
                    has_discard_date = None
                else:
                    if not case.latitude or case.death_address != row['death_address']:
                        row['latitude'], row['longitude'] = get_coordinates(row['death_address'],
                                                                            row['death_zip'])

                    # This will set any previously imported cases that are not currently
                    # eligible for discard.
                    if not case.discard_eligible:
                        row['discard_eligible'] = 'No'
                    has_discard_date = case.discard_date

                if row['manner_of_death'] in ['Homicide', 'Undetermined']: # should we include 'Suspicious' somehow?
                    if hom_retention_policy_id:
                        row['retention_policy'] = hom_retention_policy_id
                else:
                    row['retention_policy'] = non_retention_policy_id
                
                ret_length = RetentionPolicies.query.filter_by(id=row['retention_policy']).first().retention_length
                
                if ret_length and not has_discard_date:
                    # For ADMIN REVIEW cases, base it on now if closed (i.e. MOD is not NULL and ).
                    if row['autopsy_type'] == 'ADMINISTRATIVE REVIEW' and not pd.isnull(row['manner_of_death']) and (not pd.isnull(row['cod_a']) and row['cod_a'] != 'PENDING'):
                        case_close_date = datetime.combine(datetime.now().date(), datetime.min.time())
                        row['case_close_date'] = case_close_date
                        row['discard_date'] = case_close_date + timedelta(days=ret_length)
                    # For non-ADMIN REVIEW cases, base it on fa_OR_importDate from the _A autopsy report.
                    elif imported_A_date:
                        # parsed_date = datetime.strptime(imported_A_date, "%Y-%m-%d").date()
                        case_close_date = datetime.combine(imported_A_date, datetime.min.time())
                        row['case_close_date'] = case_close_date
                        row['discard_date'] = case_close_date + timedelta(days=ret_length)
                           
                # If medical record is numerical (i.e. a float) coerce into int then string
                if (not pd.isnull(row['medical_record'])) and (isinstance(row['medical_record'], float)):
                    row['medical_record'] = str(int(row['medical_record']))

                # Calculate the age_years, age_months, age_data of the decedent and give the age_string
                for key, val in calculate_age(row['date_of_birth'], row['date_of_incident']).items():
                    row[key] = val

                   
                # Set the home status
                addr = row['home_address']
                if isinstance(addr, str) and addr:
                    if addr.lower().startswith('n'):
                        row['home_status'] = 'NFA'
                    elif addr.lower().startswith('u'):
                        row['home_status'] = 'UNK'
                    else:
                        row['home_status'] = np.nan

            elif df_name == 'narratives':
                is_case_newnarrative = False
                if not pd.isnull(row['narrative']): # its in our actual import.csv

                    table = Narratives
                    item_name = 'Narratives'

                    row['case_id'] = case.id
                    narrative_type = row['narrative_type']

                    # unlike in df cases and PE, the narrative_item is called here and modified IN PLACE to maintain the id (to keep comments with it)
                    narrative_item = Narratives.query.filter(
                        Narratives.case_id == case.id,
                        Narratives.narrative_type == narrative_type,
                    ).first()

                    if narrative_item:
                        if narrative_item.checked:
                            highlights = set(re.findall(r"<mark>(.*?)</mark>", narrative_item.narrative))

                            # Transfer highlights into the new narrative
                            for phrase in highlights:
                                if phrase in row['narrative']:
                                    row['narrative'] = row['narrative'].replace(phrase, f"<mark>{phrase}</mark>")
                        
                        row['narrative'] = row['narrative'].replace("\r\n", '<br>')
                        row['narrative'] = row['narrative'].replace("\n", '<br>')
                        narrative_item.narrative = row['narrative']
                        narrative_item.updated_date = datetime.now()
                        # Adds narrative and its data IN PLACE
                        db.session.add(narrative_item)
                        db.session.commit()
                    else:
                        is_case_newnarrative = True
                    case = None
                
            elif df_name == 'personal_effects':
                is_case_newnarrative = False
                
                # Insert/update FA Personal Effects rows keyed by case_id
                table = FAPersonalEffects
                item_name = 'FAPersonalEffects'

                if not case:
                    # No case to attach to
                    continue

                # Clear previous synced effects only once per case (idempotent import)
                if case.id not in pe_cleared_cases:
                    FAPersonalEffects.query.filter_by(case_id=case.id).delete()
                    pe_cleared_cases.add(case.id)

                # If no prefixed PE columns are present for this file/row, skip cleanly
                if not pe_present_cols:
                    continue
                # If all prefixed PE fields are blank/null for this row, skip it
                if all(pd.isna(row[c]) or row[c] in (None, '') for c in pe_present_cols):
                    continue

                # Build a payload for this row from present **prefixed-only** PE columns
                payload = {
                    'case_id': case.id,
                    'db_status': 'Active',
                    'created_by': current_initials if 'current_initials' in locals() else 'ZZZ',
                    'create_date': datetime.now()
                }

                has_any_val = False
                for src_col in pe_present_cols:                # <- ONLY pe_* columns
                    model_field = PE_FIELDS[src_col]           # map pe_* -> model field name
                    val = row.get(src_col)

                    if pd.isnull(val):
                        val = None

                    # Coercions
                    if model_field == 'quantity':
                        try:
                            val = None if (val is None or (isinstance(val, float) and pd.isna(val))) else int(float(val))
                        except Exception:
                            val = None

                    elif model_field in ('received_date', 'released_date'):
                        try:
                            v = pd.to_datetime(val, errors='coerce')
                            val = None if pd.isna(v) else v.to_pydatetime()
                        except Exception:
                            val = None

                    elif model_field == 'received_by':
                        if isinstance(val, str):
                            s = val.strip()
                            if s:
                                try:
                                    # get_ids returns (personnel_id, division_id, ...) — take the first
                                    val = get_ids(s)[0]
                                except Exception as e:
                                    # print(f"[FA IMPORT][WARN] received_by lookup failed for '{s}': {e}")
                                    val = None
                            else:
                                val = None

                    if isinstance(val, str):
                        val = val.strip()

                    payload[model_field] = val
                    if val not in (None, '', np.nan):
                        has_any_val = True

                if has_any_val:
                    # Adds new PE and its data
                    db.session.add(table(**payload)) # PE is added through automation via FA Sync with created_by as ZZZ
                    db.session.commit()

                # Optional audit (matches your style elsewhere)
                record_id = case.id
                continue

            if case:
                record_id = case.id
            else:
                record_id = table.get_next_id()

            # Iterate through each of the items in the row
            # Set nan values to None
            for col, val in row.iteritems():
                if pd.isnull(val):
                    val = None
                if isinstance(val, str):
                    val = val.replace("\r\n", '<br>')
                    val = val.replace("\n", '<br>')
                    # val = val.strip()
                # if the case exists, update the value, else add the value
                # to the field_data.
                if df_name == 'cases':
                    if case and Specimens.query.filter_by(case_id=case.id).count():
                        # most imported fields are EXCLUDED from Modifications tracking, only those in add_form are modification-tracked
                        if col in case_form_fields: 
                            field = add_form[col]
                            revision = Modifications.get_next_revision(item_name, record_id, field.name)

                            # passing in the current case ORM object, date of birth field, val incoming from FA
                            new_value, new_value_text, original_value, original_value_text = get_values(case,
                                                                                                        field,
                                                                                                        val)

                            if not revision:
                                original_value = None
                                original_value_text = None


                            new_data = True
                            if new_value == original_value:  ## TODO FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior.
                                ## In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead
                                new_data = False
                            

                            if new_data:

                                prev_mod = Modifications.query.filter_by(
                                    table_name='Cases',
                                    record_id=str(record_id),
                                    field_name=field.name,
                                    revision=revision - 1
                                ).first()

                                if prev_mod:
                                    prev_mod.status = 'Revised'

                                modification = Modifications(
                                    event='IMPORTED',
                                    status="Imported",
                                    table_name='Cases',
                                    record_id=str(record_id),
                                    revision=revision,
                                    field=field.label.text,
                                    field_name=field.name,
                                    original_value=original_value,
                                    original_value_text=original_value_text,
                                    new_value=new_value,
                                    new_value_text=new_value_text,
                                    submitted_by=current_id,
                                    submitted_date=datetime.now(),
                                    reviewed_by=current_id,
                                    review_date=datetime.now()
                                )
                                db.session.add(modification)

                if case:
                    setattr(case, col, val)
                else:
                    field_data[col] = val
                    item = True

            revision = Modifications.get_next_revision(item_name, str(record_id), 'file_name')
            modification = Modifications(
                event='IMPORTED',
                status="Imported",
                table_name=item_name,
                record_id=str(record_id),
                revision=revision,
                field="File",
                field_name="file_name",
                original_value=filename,
                original_value_text=filename,
                new_value=savename,
                new_value_text=savename,
                submitted_by=current_id,
                submitted_date=datetime.now(),
                reviewed_by=current_id,
                review_date=datetime.now()
            )

            db.session.add(modification)

            if item:
                # defines item ONLY for Cases and new Narratives, avoids duplicate adds for existing Narratives and PE
                if is_case_newnarrative is True:
                    item = table(**field_data)            
                    db.session.add(item)
                else:
                    continue
            
            # collects previous session adds (Cases, Modifications; excl existing Narratives and PE, added earlier)
            db.session.commit()

    try:
        for case_number in cases_touched:
            c = Cases.query.filter_by(case_number=case_number).first()
            if not c:
                continue

            # Condition A (existing): Autopsy upload present
            has_autopsy_upload = _has_autopsy_upload(c.id)

            # Condition B (new): Administrative Review AND COD_A has finalized
            is_admin_review = isinstance(c.autopsy_type, str) and c.autopsy_type.strip().lower() == 'administrative review'
            admin_review_ready = is_admin_review and _cod_a_finalized(getattr(c, 'cod_a', None))

            # Only create _O1 if none exists yet
            if (has_autopsy_upload or admin_review_ready) and not _has_decedent_record(c):
                print(f"[FA IMPORT] Decedent report trigger: case {c.case_number} "
                      f"(autopsy_upload={has_autopsy_upload}, admin_review_ready={admin_review_ready})")
                generate_decedent_report(c.id)
    except Exception as e:
        print(f"[FA IMPORT] Decedent report trigger failed: {e}")

    import_time = datetime.now() - start_import
    minutes, seconds = divmod(import_time.total_seconds(), 60)
    milliseconds = import_time.microseconds // 1000  # Convert microseconds to milliseconds
    formatted_time = f"{int(minutes)}m {int(seconds)}s {milliseconds}ms"
    # flash(Markup(f'<b>{filename}</b> successfully imported! Import took {formatted_time}'), 'success')
    print("[FA IMPORT] IMPORT FINISHED: ", datetime.now() - start_import, " aka ", formatted_time)
    print(f"[FA IMPORT {datetime.now()}] <<<<<< EXITED process_import_file")


def auto_import_fa_cases(log_path, fa_col_map, impl_date, add_form):
    print(f'[{datetime.now()}] >>> ENTERED auto_import_fa_cases')
    # if not os.path.exists(log_path):
    #     print("Log file not found.")
    # else:
    #     print('log_path found')

    df = pd.read_csv(log_path, dtype=str)
    df = df.fillna("")
    updated = False

    for idx, row in df[(df["LIMS_Imported"] == "") & (df["Status"] == "Success")].iterrows():
        file_path = row["OutputFile"]
        filename = os.path.basename(file_path)
        savename = f"{filename.split('.')[0]}_{datetime.now().strftime('%Y%m%d%H%M')}.csv"
        print(f"[{datetime.now()}] PREPARING to import: {filename}")
        if not os.path.exists(file_path):
            df.at[idx, "LIMS_Error"] = f"Missing file: {file_path}"
            continue

        try:
            import_start = datetime.now()
            df.at[idx, "LIMS_Imported"] = import_start.strftime("%Y-%m-%dT%H:%M:%S")

            with app.app_context():
                path = os.path.join(current_app.config['FILE_SYSTEM'], 'imports', savename)
                shutil.copy(file_path, path)
                process_import_file(file_path, filename, savename, fa_col_map, impl_date, add_form)
                print(f"[{datetime.now()}] Imported {filename}")

            end_time = datetime.now()
            df.at[idx, "LIMS_Completed"] = end_time.strftime("%Y-%m-%dT%H:%M:%S")
            df.at[idx, "LIMS_Error"] = ""
        except Exception as e:
            print(f"Exception occurred: {str(e)}")
            df.at[idx, "LIMS_Error"] = str(e)

        updated = True

    if updated:
        try:
            # Re-read the latest version of the log file
            df_latest = pd.read_csv(log_path, dtype=str).fillna("")

            # Build sets of OutputFile values
            processed_files = set(df["OutputFile"])
            latest_files = set(df_latest["OutputFile"])

            # Keep rows added externally (i.e., not in the original df)
            new_external_rows = df_latest[~df_latest["OutputFile"].isin(processed_files)]

            # Combine: processed rows (df) + new rows added during processing
            combined = pd.concat([df, new_external_rows], ignore_index=True)

            # Drop duplicates, keeping the *last* occurrence (yours overwrites if duplicate)
            combined = combined.drop_duplicates(subset=["OutputFile"], keep="last")

            # Write safely
            try:
                combined.to_csv(log_path, index=False, encoding="utf-8")
            except Exception as e:
                print(f"First attempt failed: {e}, retrying in 2 seconds...")
                time.sleep(2)
                combined.to_csv(log_path, index=False, encoding="utf-8")

        except Exception as e:
            print(f"❌ Failed to safely merge updated log: {e}")
    else:
        print("ℹ️ Auto-import found nothing to process.")

    print(f"[{datetime.now()}] <<< EXITED auto_import_fa_cases")
    return None


def generate_decedent_report(case_id: int, *, _in_worker: bool = False):
    from pathlib import Path   
    # App mode: enqueue this same function into the single-worker queue
    if not _in_worker:
        return submit("lims.cases.functions:generate_decedent_report",
                      priority=10, case_id=case_id, _in_worker=True)

    # Worker mode: actual generation (runs in the queue process)
    with app.app_context():
        case = Cases.query.filter_by(id=case_id).first()
        app_root = Path(current_app.root_path)

        template_path = app_root / "static" / "filesystem" / "decedent_report_template" / "ocme_decedent_report_template.docx"
        records_dir = app_root / "static" / "filesystem" / "records" / case.case_number
        records_dir.mkdir(parents=True, exist_ok=True)

        doc = DocxTemplate(str(template_path))

        # ----- height formatting -----
        h = case.height_inches
        if h in (None, ""):
            height_str = ""
        else:
            try:
                total_inches = int(round(float(h)))
                feet, inches = divmod(total_inches, 12)
                height_str = f"{feet}' {inches}\""
            except (TypeError, ValueError):
                height_str = ""

        summary_row = (Narratives.query
                       .filter_by(case_id=case_id, narrative_type="Summary (AI)")
                       .first())
        summary_ai_text = (summary_row.narrative or "").strip() if summary_row else ""

        relationship_str = ""
        if case.next_of_kin_relationship:
            m = re.search(r'\((.*?)\)', case.next_of_kin_relationship)
            if m:
                relationship_str = m.group(1).strip()

        data = {
            "case_number": case.case_number,
            "full_name": f"{case.last_name}, {case.first_name}",
            "alias": case.alias_names if case.alias_names else "",
            "date_of_death": case.date_of_incident.strftime("%m/%d/%Y") if case.date_of_incident else "",
            "date_of_birth": case.date_of_birth.strftime("%m/%d/%Y") if case.date_of_birth else "",
            "age": case.age_years or "",
            "height": height_str,
            "weight": f"{case.weight_pounds}lbs" if case.weight_pounds else "",
            "gender": case.gender.name if case.gender else "",
            "pronouncement_date": case.date_of_incident.strftime("%m/%d/%Y") if case.date_of_incident else "",
            "pronouncement_time": case.time_of_incident if case.time_of_incident else "",
            "death_premises": case.death_premises or "",
            "death_address": f"{case.death_address}" if case.death_address else "",
            "cod_a": case.cod_a or "",
            "cod_b": case.cod_b or "",
            "cod_c": case.cod_c or "",
            "cod_d": case.cod_d or "",
            "other_conditions": case.other_conditions if case.other_conditions and case.other_conditions.lower() != 'none' else "",
            "manner_of_death": case.manner_of_death or "",
            "autopsy_type": case.autopsy_type or "",
            "primary_pathologist": case.pathologist.full_name if case.pathologist else "",
            "investigators": case.investigator.full_name if case.investigator else "",
            "next_of_kin": relationship_str,
            "nok_notify_date": case.nok_notify_date.strftime('%m/%d/%Y') if case.nok_notify_date else "",
            "body_received": case.fa_cooler_datetime.strftime('%m/%d/%Y') if case.fa_cooler_datetime else "",
            "mortuary": case.mortuary if case.mortuary else "",
            "body_released": case.autopsy_finalized_date.strftime('%m/%d/%Y') if case.autopsy_finalized_date else "",
            "public_administrator": case.public_administrator if case.public_administrator else "",
            "pa_notified_date": case.pa_notified_date.strftime('%m/%d/%Y') if case.pa_notified_date else "",
            "summary_ai": summary_ai_text,
        }

        # --- Initial narrative: preserve <br> and underline headers ---
        initial_row = (Narratives.query
                       .filter_by(case_id=case.id, narrative_type="Initial")
                       .order_by(Narratives.id.desc())
                       .first())
        raw_initial = (initial_row.narrative or "") if initial_row else ""
        cleaned = raw_initial.replace("<mark>", "").replace("</mark>", "")
        normalized = re.sub(r'(?i)<br\s*/?>', '<br>', cleaned).strip()
        tokens = re.split(r'(?i)(<br>)', normalized)

        HEADER_RE = re.compile(r'^[A-Z0-9 ()/\-.,:&]+$')
        def is_header(line: str) -> bool:
            s = line.strip()
            return bool(s) and (s.endswith(':') or (HEADER_RE.match(s) and s.upper() == s))

        rt = RichText()
        buffer_line = ""

        def flush_line():
            nonlocal buffer_line
            line = buffer_line
            buffer_line = ""
            if line:
                rt.add(line, underline=True) if is_header(line) else rt.add(line)

        for tok in tokens:
            if tok.lower() == "<br>":
                flush_line()
                rt.add("\n")
            else:
                buffer_line += tok
        if buffer_line:
            flush_line()

        data["initial"] = rt

        if case.middle_name:
            data['full_name'] += f" {case.middle_name}"
        if case.death_zip:
            data['death_address'] += f" {case.death_zip}"
        if case.investigator_second:
            data['investigators'] += f", {case.investigator_second.full_name}"

        # Determine next O#
        max_n = 0
        for entry in records_dir.iterdir():
            m = re.match(rf'^{re.escape(case.case_number)}_O(\d+)\.(?:docx|pdf)$', entry.name, re.IGNORECASE)
            if m:
                try:
                    max_n = max(max_n, int(m.group(1)))
                except ValueError:
                    pass
        inc_num = max_n + 1

        base_name = f"{case.case_number}_O{inc_num}"
        docx_path = records_dir / f"{base_name}.docx"
        pdf_path  = records_dir / f"{base_name}.pdf"

        # DB record
        # This does not use add_item since there is no form submission
        new_rec = Records(
            case_id=case.id,
            record_name=base_name,
            record_type=14,
            record_number=inc_num,
            db_status='Active',
            created_by='ZZZ',
            create_date=datetime.now(),
            locked=False,
            revision=False
        )

        db.session.add(new_rec) # record is generated through automation with created_by as ZZZ
        db.session.flush()  # get new_rec.id without committing yet

        mod = Modifications(
            event='CREATED',
            status='Approved',
            table_name='Records',
            record_id=new_rec.id,
            revision=0,
            field='record_name',
            field_name='record_name',
            new_value=new_rec.record_name,
            new_value_text=new_rec.record_name,
            submitted_by=45,             # no user in worker
            submitted_date=datetime.now(),
            reviewed_by=45,
            review_date=datetime.now()
        )
        db.session.add(mod)
        db.session.commit()
        
        def _coerce_qty(val):
            if val is None:
                return ""
            s = str(val).strip()
            if s == "":
                return ""
            try:
                f = float(s)
                return int(f) if f.is_integer() else f
            except Exception:
                return s

        type_col = getattr(FAPersonalEffects, 'type')  # 'type' can be awkward; use getattr

        pe_rows = (
            db.session.query(FAPersonalEffects)
            .filter(
                FAPersonalEffects.case_id == case.id,  # use the current case’s id
                or_(FAPersonalEffects.db_status != 'Removed',
                    FAPersonalEffects.db_status.is_(None)),
                # normalize: trim + lower, then compare to lowercase literal
                func.lower(func.ltrim(func.rtrim(type_col))) == 'property'
            )
            .order_by(FAPersonalEffects.id.asc())
            .all()
        )

        print(f"[Decedent] {case.case_number}: Property PE rows = {len(pe_rows)}")

        items = []
        for pe in pe_rows:
            qty = _coerce_qty(pe.quantity)
            desc = (pe.description or "").strip()
            if (qty == "" or qty == 0) and desc == "":
                continue
            items.append({"qty": qty, "desc": desc})

        data["property_items"] = items
                # Render -> DOCX
        doc.render(data)
        doc.save(str(docx_path))

        # Convert DOCX -> PDF (COM handled by queue)
        convert(str(docx_path), str(pdf_path))

        try:
            os.remove(str(docx_path))
        except OSError:
            pass

        return str(pdf_path), inc_num
    
    
def generate_decedent_report_draft(case_id: int, *, _in_worker: bool = False):
    """
    Build a Decedent Report PDF for the given case via the queue but NEVER create/update any DB Records.
    Returns (pdf_path, sequence_number) from the worker so the route can stream the file and then delete it.
    """
    # App mode: enqueue this same function into the single-worker queue
    if not _in_worker:
        # IMPORTANT: We expect submit() to return a Future-like object with .result()
        return submit("lims.cases.functions:generate_decedent_report_draft",
                      priority=10, case_id=case_id, _in_worker=True)

    # Worker mode: actual generation (runs in the queue process)
    with app.app_context():
        case = Cases.query.filter_by(id=case_id).first()
        if not case:
            raise RuntimeError(f"Case id {case_id} not found")

        app_root = Path(current_app.root_path)

        template_path = app_root / "static" / "filesystem" / "decedent_report_template" / "ocme_decedent_report_draft_template.docx"
        if not template_path.is_file():
            raise RuntimeError(f"Template not found at {template_path}")

        # We still inspect the records dir only to find “next O#” number,
        # but we DO NOT create a Records row.
        records_dir = app_root / "static" / "filesystem" / "records" / (case.case_number or str(case.id))
        records_dir.mkdir(parents=True, exist_ok=True)

        doc = DocxTemplate(str(template_path))

        # ----- height formatting -----
        h = case.height_inches
        if h in (None, ""):
            height_str = ""
        else:
            try:
                total_inches = int(round(float(h)))
                feet, inches = divmod(total_inches, 12)
                height_str = f"{feet}' {inches}\""
            except (TypeError, ValueError):
                height_str = ""

        summary_row = (Narratives.query
                       .filter_by(case_id=case_id, narrative_type="Summary (AI)")
                       .first())
        summary_ai_text = (summary_row.narrative or "").strip() if summary_row else ""

        relationship_str = ""
        if case.next_of_kin_relationship:
            m = re.search(r'\((.*?)\)', case.next_of_kin_relationship)
            if m:
                relationship_str = m.group(1).strip()

        data = {
            "case_number": case.case_number,
            "full_name": f"{case.last_name}, {case.first_name}",
            "alias": case.alias_names if case.alias_names else "",
            "date_of_death": case.date_of_incident.strftime("%m/%d/%Y") if case.date_of_incident else "",
            "date_of_birth": case.date_of_birth.strftime("%m/%d/%Y") if case.date_of_birth else "",
            "age": case.age_years or "",
            "height": height_str,
            "weight": f"{case.weight_pounds}lbs" if case.weight_pounds else "",
            "gender": case.gender.name if getattr(case, "gender", None) else "",
            "pronouncement_date": case.date_of_incident.strftime("%m/%d/%Y") if case.date_of_incident else "",
            "pronouncement_time": case.time_of_incident if case.time_of_incident else "",
            "death_premises": case.death_premises or "",
            "death_address": f"{case.death_address}" if case.death_address else "",
            "cod_a": case.cod_a or "",
            "cod_b": case.cod_b or "",
            "cod_c": case.cod_c or "",
            "cod_d": case.cod_d or "",
            "other_conditions": case.other_conditions if case.other_conditions and case.other_conditions.lower() != 'none' else "",
            "manner_of_death": case.manner_of_death or "",
            "autopsy_type": case.autopsy_type or "",
            "primary_pathologist": case.pathologist.full_name if getattr(case, "pathologist", None) else "",
            "investigators": case.investigator.full_name if getattr(case, "investigator", None) else "",
            "next_of_kin": relationship_str,
            "nok_notify_date": case.nok_notify_date.strftime('%m/%d/%Y') if case.nok_notify_date else "",
            "body_received": case.fa_cooler_datetime.strftime('%m/%d/%Y') if case.fa_cooler_datetime else "",
            "mortuary": case.mortuary if case.mortuary else "",
            "body_released": case.autopsy_finalized_date.strftime('%m/%d/%Y') if case.autopsy_finalized_date else "",
            "public_administrator": case.public_administrator if case.public_administrator else "",
            "pa_notified_date": case.pa_notified_date.strftime('%m/%d/%Y') if case.pa_notified_date else "",
            "summary_ai": summary_ai_text,
        }

        # ----- build 'Initial' narrative with simple formatting -----
        initial_row = (Narratives.query
                       .filter_by(case_id=case.id, narrative_type="Initial")
                       .order_by(Narratives.id.desc())
                       .first())
        raw_initial = (initial_row.narrative or "") if initial_row else ""
        cleaned = raw_initial.replace("<mark>", "").replace("</mark>", "")
        normalized = re.sub(r'(?i)<br\s*/?>', '<br>', cleaned).strip()
        tokens = re.split(r'(?i)(<br>)', normalized)

        HEADER_RE = re.compile(r'^[A-Z0-9 ()/\-.,:&]+$')
        def is_header(line: str) -> bool:
            s = line.strip()
            return bool(s) and (s.endswith(':') or (HEADER_RE.match(s) and s.upper() == s))

        rt = RichText()
        buffer_line = ""

        def flush_line():
            nonlocal buffer_line
            line = buffer_line
            buffer_line = ""
            if line:
                rt.add(line, underline=True) if is_header(line) else rt.add(line)

        for tok in tokens:
            if tok.lower() == "<br>":
                flush_line()
                rt.add("\n")
            else:
                buffer_line += tok
        if buffer_line:
            flush_line()

        data["initial"] = rt

        if case.middle_name:
            data['full_name'] += f" {case.middle_name}"
        if case.death_zip:
            data['death_address'] += f" {case.death_zip}"
        if getattr(case, "investigator_second", None):
            data['investigators'] += f", {case.investigator_second.full_name}"

        # Determine next O# (based on existing files), but we'll write to a TEMP directory
        max_n = 0
        for entry in records_dir.iterdir():
            m = re.match(rf'^{re.escape(case.case_number)}_O(\d+)\.(?:docx|pdf)$', entry.name, re.IGNORECASE)
            if m:
                try:
                    max_n = max(max_n, int(m.group(1)))
                except ValueError:
                    pass
        inc_num = max_n + 1

        # --- Write to a temp dir (NO DB writes) ---
        tmp_dir = Path(tempfile.mkdtemp(prefix="dec_report_", dir=str(records_dir)))
        base_name = f"{case.case_number}_O1"
        docx_path = tmp_dir / f"{base_name}.docx"
        pdf_path  = tmp_dir / f"{base_name}.pdf"

        # Render -> DOCX
        doc.render(data)
        doc.save(str(docx_path))

        # Convert DOCX -> PDF
        convert(str(docx_path), str(pdf_path))

        # Clean the DOCX (keep only the PDF for download)
        try:
            os.remove(str(docx_path))
        except OSError:
            pass

        # Return path for the route to stream & remove
        return str(pdf_path), inc_num
